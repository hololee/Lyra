from fastapi import APIRouter, Depends, HTTPException, Query, Request, status
from fastapi.responses import RedirectResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import text
from types import SimpleNamespace
from typing import List
from uuid import UUID
from ..database import get_db
from ..models import Environment, Setting, WorkerServer
from ..core.security import SecretCipherError, SecretKeyError, encrypt_secret
from ..core.worker_registry import (
    WORKER_HEALTH_HEALTHY,
    WorkerRequestError,
    call_worker_api,
    refresh_worker_health,
)
from ..schemas import (
    CustomPortAllocateRequest,
    CustomPortAllocateResponse,
    CustomPortMapping,
    EnvironmentCreate,
    EnvironmentResponse,
)
from ..tasks import create_environment_task
import docker
import secrets
import time
import random
import logging
from sqlalchemy.exc import IntegrityError
import json
from urllib.parse import urlsplit, urlunsplit

router = APIRouter(
    prefix="/environments",
    tags=["environments"],
)

JUPYTER_LAUNCH_TTL_SECONDS = 60
jupyter_launch_tickets: dict[str, dict[str, str | float | bool]] = {}
CODE_LAUNCH_TTL_SECONDS = 60
code_launch_tickets: dict[str, dict[str, str | float | bool]] = {}
MAX_PORT_ALLOCATION_RETRIES = 8
CUSTOM_HOST_PORT_RANGE = (35001, 60000)
CUSTOM_CONTAINER_PORT_RANGE = (10000, 20000)
RESERVED_CONTAINER_PORTS = {22, 8080, 8888}
GPU_ALLOCATION_LOCK_KEY = 93821
GPU_OCCUPIED_STATUSES = {"creating", "building", "running", "starting"}
REMOTE_SURROGATE_PORT_RANGE = (61001, 65535)
logger = logging.getLogger(__name__)
BUILD_ERROR_SETTING_PREFIX = "build_error:"


def _is_name_unique_violation(error: IntegrityError) -> bool:
    text = f"{error}".lower()
    if error.orig is not None:
        text += f" {error.orig}".lower()
    return "environments_name_key" in text or ("duplicate key value" in text and "(name)" in text)


def _is_port_unique_violation(error: IntegrityError) -> bool:
    text = f"{error}".lower()
    if error.orig is not None:
        text += f" {error.orig}".lower()
    return any(
        key in text
        for key in (
            "environments_ssh_port_key",
            "environments_jupyter_port_key",
            "environments_code_port_key",
            "(ssh_port)",
            "(jupyter_port)",
            "(code_port)",
        )
    )


def _is_worker_environment_not_found(error: WorkerRequestError) -> bool:
    if error.status_code == 404:
        return True
    if error.code in {"environment_not_found", "worker_environment_not_found"}:
        return True
    if error.code != "worker_request_failed":
        return False
    message = (error.message or "").lower()
    return "not found" in message or "environment not found" in message


async def _is_worker_environment_absent(worker: WorkerServer, environment_id: str) -> bool:
    try:
        await call_worker_api(
            worker,
            method="GET",
            path=f"/api/worker/environments/{environment_id}",
        )
        return False
    except WorkerRequestError as error:
        return _is_worker_environment_not_found(error)


def _cleanup_expired_jupyter_tickets():
    now = time.time()
    expired = [
        ticket
        for ticket, meta in jupyter_launch_tickets.items()
        if meta.get("used") or float(meta.get("expires_at", 0)) < now
    ]
    for ticket in expired:
        jupyter_launch_tickets.pop(ticket, None)


def _cleanup_expired_code_tickets():
    now = time.time()
    expired = [
        ticket
        for ticket, meta in code_launch_tickets.items()
        if meta.get("used") or float(meta.get("expires_at", 0)) < now
    ]
    for ticket in expired:
        code_launch_tickets.pop(ticket, None)


def _build_worker_service_url(base_url: str, service_port: int, launch_path: str) -> str:
    parsed_base = urlsplit(str(base_url or "").strip())
    parsed_launch = urlsplit(str(launch_path or "").strip())

    base_path = (parsed_base.path or "").rstrip("/")
    launch_only_path = parsed_launch.path or "/"
    if not launch_only_path.startswith("/"):
        launch_only_path = f"/{launch_only_path}"
    combined_path = f"{base_path}{launch_only_path}" if base_path else launch_only_path

    if parsed_base.scheme and parsed_base.hostname:
        hostname = parsed_base.hostname
        # Bracket IPv6 hostnames when building netloc manually.
        if ":" in hostname and not hostname.startswith("["):
            hostname = f"[{hostname}]"
        host_with_port = hostname if service_port <= 0 else f"{hostname}:{service_port}"
        if parsed_base.username:
            auth = parsed_base.username
            if parsed_base.password:
                auth = f"{auth}:{parsed_base.password}"
            netloc = f"{auth}@{host_with_port}"
        else:
            netloc = host_with_port
        return urlunsplit((parsed_base.scheme, netloc, combined_path, parsed_launch.query, parsed_launch.fragment))

    normalized = str(base_url or "").rstrip("/")
    suffix = combined_path
    if parsed_launch.query:
        suffix = f"{suffix}?{parsed_launch.query}"
    if parsed_launch.fragment:
        suffix = f"{suffix}#{parsed_launch.fragment}"
    return f"{normalized}{suffix}"


def _parse_worker_service_port(raw_port) -> int | None:
    if isinstance(raw_port, int):
        return raw_port if raw_port > 0 else None
    if isinstance(raw_port, str):
        value = raw_port.strip()
        if value.isdigit():
            parsed = int(value)
            return parsed if parsed > 0 else None
    return None


def _is_host_environment_running_now(env: Environment) -> bool:
    container_name = f"lyra-{env.name}-{env.id}"
    try:
        client = docker.from_env()
        container = client.containers.get(container_name)
        return container.status == "running"
    except docker.errors.NotFound:
        return False
    except Exception as error:  # noqa: BLE001
        logger.warning("Failed to check runtime status for environment %s: %s", env.id, error)
        # Fallback to DB status when Docker lookup is temporarily unavailable.
        return env.status == "running"


def _format_container_state_summary(container) -> str:
    state = container.attrs.get("State", {}) if container is not None else {}
    exit_code = state.get("ExitCode")
    oom_killed = state.get("OOMKilled")
    error_msg = str(state.get("Error", "") or "").strip()
    finished_at = str(state.get("FinishedAt", "") or "").strip()
    status = str(state.get("Status", "") or "").strip() or str(getattr(container, "status", "") or "").strip()

    details = [
        "[Container Diagnostics]",
        f"Status: {status or 'unknown'}",
        f"ExitCode: {exit_code if exit_code is not None else 'unknown'}",
        f"OOMKilled: {oom_killed if oom_killed is not None else 'unknown'}",
    ]
    if finished_at and finished_at != "0001-01-01T00:00:00Z":
        details.append(f"FinishedAt: {finished_at}")
    if error_msg:
        details.append(f"Error: {error_msg}")
    return "\n".join(details)


def _detect_total_gpus() -> int:
    import pynvml

    try:
        pynvml.nvmlInit()
        total_gpus = pynvml.nvmlDeviceGetCount()
        pynvml.nvmlShutdown()
        return int(total_gpus)
    except Exception as exc:
        raise HTTPException(status_code=500, detail="Failed to detect GPUs on host") from exc


async def _collect_used_gpu_indices(db: AsyncSession, worker_server_id: UUID | None = None) -> set[int]:
    stmt = select(Environment).where(Environment.status.in_(GPU_OCCUPIED_STATUSES))
    if worker_server_id is None:
        stmt = stmt.where(Environment.worker_server_id.is_(None))
    else:
        stmt = stmt.where(Environment.worker_server_id == worker_server_id)
    result = await db.execute(stmt)
    active_envs = result.scalars().all()
    used_indices: set[int] = set()
    for active in active_envs:
        if active.status not in GPU_OCCUPIED_STATUSES:
            continue
        if active.gpu_indices:
            used_indices.update(active.gpu_indices)
    return used_indices


async def _get_jupyter_token(db: AsyncSession, environment_id: str) -> str | None:
    key = f"jupyter_token:{environment_id}"
    result = await db.execute(select(Setting).where(Setting.key == key))
    token_setting = result.scalars().first()
    return token_setting.value if token_setting else None


async def _get_build_error_message(db: AsyncSession, environment_id: str) -> str | None:
    key = f"{BUILD_ERROR_SETTING_PREFIX}{environment_id}"
    result = await db.execute(select(Setting).where(Setting.key == key))
    setting = result.scalars().first()
    if not setting:
        return None
    value = str(setting.value or "").strip()
    return value or None


def _normalize_custom_ports(raw_ports) -> list[dict]:
    if not raw_ports:
        return []
    normalized = []
    for item in raw_ports:
        if isinstance(item, CustomPortMapping):
            host_port = int(item.host_port)
            container_port = int(item.container_port)
        elif isinstance(item, dict):
            host_port = int(item.get("host_port"))
            container_port = int(item.get("container_port"))
        else:
            continue
        normalized.append({"host_port": host_port, "container_port": container_port})
    return normalized


async def _get_custom_ports_map(db: AsyncSession) -> dict[str, list[dict]]:
    result = await db.execute(select(Setting).where(Setting.key.like("custom_ports:%")))
    settings = result.scalars().all()
    custom_ports_map: dict[str, list[dict]] = {}
    for setting in settings:
        try:
            env_id = setting.key.split("custom_ports:", 1)[1]
            parsed = json.loads(setting.value)
            custom_ports_map[env_id] = _normalize_custom_ports(parsed)
        except Exception:
            continue
    return custom_ports_map


async def _get_custom_ports_for_environment(db: AsyncSession, environment_id: str) -> list[dict]:
    result = await db.execute(select(Setting).where(Setting.key == f"custom_ports:{environment_id}"))
    setting = result.scalars().first()
    if not setting:
        return []
    try:
        parsed = json.loads(setting.value)
        return _normalize_custom_ports(parsed)
    except Exception:
        return []


async def _get_worker_server_by_id(db: AsyncSession, worker_server_id: UUID | str | None) -> WorkerServer | None:
    if not worker_server_id:
        return None
    worker_uuid = worker_server_id
    if isinstance(worker_server_id, str):
        try:
            worker_uuid = UUID(worker_server_id)
        except Exception:
            return None
    result = await db.execute(select(WorkerServer).where(WorkerServer.id == worker_uuid))
    return result.scalars().first()


async def _assert_worker_is_ready(db: AsyncSession, worker_server_id: UUID | str | None) -> WorkerServer:
    worker = await _get_worker_server_by_id(db, worker_server_id)
    if not worker:
        raise HTTPException(
            status_code=404,
            detail={"code": "worker_not_found", "message": "Worker server not found"},
        )
    health = await refresh_worker_health(db, worker, persist=False)
    if health.status != WORKER_HEALTH_HEALTHY:
        raise HTTPException(
            status_code=503,
            detail={"code": "worker_unreachable", "message": health.message},
        )
    return worker


def _map_worker_request_error(error: WorkerRequestError) -> HTTPException:
    return HTTPException(
        status_code=error.status_code,
        detail={"code": error.code, "message": error.message},
    )


async def _allocate_remote_surrogate_ports(db: AsyncSession) -> tuple[int, int, int]:
    blocked = await _collect_blocked_host_ports(db)
    start, end = REMOTE_SURROGATE_PORT_RANGE
    ssh_port = _pick_free_port(start, end, blocked)
    blocked.add(ssh_port)
    jupyter_port = _pick_free_port(start, end, blocked)
    blocked.add(jupyter_port)
    code_port = _pick_free_port(start, end, blocked)
    return ssh_port, jupyter_port, code_port


def _get_docker_used_ports() -> set[int]:
    used_ports: set[int] = set()
    try:
        client = docker.from_env()
        containers = client.containers.list(all=True)
        for container in containers:
            ports = container.attrs.get("NetworkSettings", {}).get("Ports", {}) or {}
            for bindings in ports.values():
                if not bindings:
                    continue
                for binding in bindings:
                    host_port = binding.get("HostPort")
                    if host_port and str(host_port).isdigit():
                        used_ports.add(int(host_port))
    except Exception:
        # Docker query failures are tolerated; runtime run() retry still handles conflicts.
        pass
    return used_ports


def _pick_free_port(start: int, end: int, blocked_ports: set[int]) -> int:
    candidates = list(range(start, end + 1))
    random.shuffle(candidates)
    for port in candidates:
        if port in blocked_ports:
            continue
        return port
    raise HTTPException(
        status_code=503,
        detail=f"No available host ports in range {start}-{end}",
    )


async def _allocate_ports(db: AsyncSession) -> tuple[int, int, int]:
    result = await db.execute(select(Environment.ssh_port, Environment.jupyter_port, Environment.code_port))
    rows = result.all()
    blocked_ports: set[int] = set()
    for ssh_port, jupyter_port, code_port in rows:
        blocked_ports.add(ssh_port)
        blocked_ports.add(jupyter_port)
        blocked_ports.add(code_port)

    custom_ports_map = await _get_custom_ports_map(db)
    for mappings in custom_ports_map.values():
        for mapping in mappings:
            host_port = mapping.get("host_port")
            if isinstance(host_port, int):
                blocked_ports.add(host_port)

    blocked_ports.update(_get_docker_used_ports())

    ssh_port = _pick_free_port(20000, 25000, blocked_ports)
    blocked_ports.add(ssh_port)
    jupyter_port = _pick_free_port(25001, 30000, blocked_ports)
    blocked_ports.add(jupyter_port)
    code_port = _pick_free_port(30001, 35000, blocked_ports)
    return ssh_port, jupyter_port, code_port


async def _collect_blocked_host_ports(db: AsyncSession) -> set[int]:
    blocked_ports: set[int] = set()
    result = await db.execute(select(Environment.ssh_port, Environment.jupyter_port, Environment.code_port))
    rows = result.all()
    for ssh_port, jupyter_port, code_port in rows:
        blocked_ports.add(ssh_port)
        blocked_ports.add(jupyter_port)
        blocked_ports.add(code_port)

    custom_ports_map = await _get_custom_ports_map(db)
    for mappings in custom_ports_map.values():
        for mapping in mappings:
            host_port = mapping.get("host_port")
            if isinstance(host_port, int):
                blocked_ports.add(host_port)

    blocked_ports.update(_get_docker_used_ports())
    return blocked_ports


def _validate_custom_ports(custom_ports: list[dict]):
    host_ports = set()
    container_ports = set()
    for mapping in custom_ports:
        host_port = mapping["host_port"]
        container_port = mapping["container_port"]
        if host_port in host_ports:
            raise HTTPException(
                status_code=400,
                detail={
                    "code": "duplicate_custom_host_port",
                    "message": f"Duplicate host port in custom mappings: {host_port}",
                },
            )
        if container_port in container_ports:
            raise HTTPException(
                status_code=400,
                detail={
                    "code": "duplicate_custom_container_port",
                    "message": f"Duplicate container port in custom mappings: {container_port}",
                },
            )
        if container_port in RESERVED_CONTAINER_PORTS:
            raise HTTPException(
                status_code=400,
                detail={
                    "code": "reserved_container_port",
                    "message": f"Container port {container_port} is reserved",
                },
            )
        host_ports.add(host_port)
        container_ports.add(container_port)


async def _allocate_custom_port_mappings(
    db: AsyncSession,
    count: int,
    current_ports: list[dict] | None = None,
) -> list[dict]:
    if count <= 0:
        return []
    blocked_host_ports = await _collect_blocked_host_ports(db)
    existing = _normalize_custom_ports(current_ports or [])
    for mapping in existing:
        blocked_host_ports.add(mapping["host_port"])

    blocked_container_ports = set(RESERVED_CONTAINER_PORTS)
    for mapping in existing:
        blocked_container_ports.add(mapping["container_port"])

    mappings: list[dict] = []
    host_start, host_end = CUSTOM_HOST_PORT_RANGE
    container_start, container_end = CUSTOM_CONTAINER_PORT_RANGE
    for _ in range(count):
        host_port = _pick_free_port(host_start, host_end, blocked_host_ports)
        blocked_host_ports.add(host_port)
        container_port = _pick_free_port(container_start, container_end, blocked_container_ports)
        blocked_container_ports.add(container_port)
        mappings.append({"host_port": host_port, "container_port": container_port})

    return mappings


@router.post("/ports/allocate", response_model=CustomPortAllocateResponse)
async def allocate_custom_ports(payload: CustomPortAllocateRequest, db: AsyncSession = Depends(get_db)):
    count = payload.count if payload.count > 0 else 1
    mappings = await _allocate_custom_port_mappings(db, count=count, current_ports=payload.current_ports)
    return {"mappings": mappings}


@router.post("/", response_model=EnvironmentResponse, status_code=status.HTTP_201_CREATED)
async def create_environment(env: EnvironmentCreate, db: AsyncSession = Depends(get_db)):
    if not env.dockerfile_content or not env.dockerfile_content.strip():
        raise HTTPException(
            status_code=400,
            detail={"code": "dockerfile_required", "message": "Dockerfile content is required"},
        )

    existing = await db.execute(select(Environment).where(Environment.name == env.name))
    if existing.scalars().first():
        raise HTTPException(
            status_code=409,
            detail={"code": "duplicate_environment_name", "message": "Environment name already exists"},
        )

    try:
        encrypted_root_password = encrypt_secret(env.root_password)
    except SecretKeyError as error:
        raise HTTPException(
            status_code=500,
            detail={"code": "security_key_missing", "message": str(error)},
        ) from error
    except SecretCipherError as error:
        raise HTTPException(
            status_code=500,
            detail={"code": "password_encryption_failed", "message": str(error)},
        ) from error

    if env.worker_server_id:
        worker = await _assert_worker_is_ready(db, env.worker_server_id)
        worker_snapshot = SimpleNamespace(
            id=worker.id,
            name=worker.name,
            base_url=worker.base_url,
            api_token_encrypted=worker.api_token_encrypted,
        )
        worker_id = worker.id
        remote_payload = {
            "name": env.name,
            "container_user": env.container_user,
            "dockerfile_content": env.dockerfile_content,
            "enable_jupyter": env.enable_jupyter,
            "enable_code_server": env.enable_code_server,
            "mount_config": [m.model_dump() for m in env.mount_config],
            "custom_ports": [p.model_dump() for p in env.custom_ports],
            "gpu_count": env.gpu_count,
            "selected_gpu_indices": env.selected_gpu_indices,
            "root_password": env.root_password,
            "worker_server_id": None,
        }
        try:
            remote_env = await call_worker_api(
                worker_snapshot,
                method="POST",
                path="/api/worker/environments",
                payload=remote_payload,
            )
        except WorkerRequestError as error:
            raise _map_worker_request_error(error) from error

        try:
            remote_env_id = UUID(str(remote_env.get("id", "")))
        except Exception as error:
            raise HTTPException(
                status_code=502,
                detail={"code": "worker_api_mismatch", "message": "Worker response did not include a valid id"},
            ) from error

        async def _cleanup_remote_environment() -> None:
            try:
                await call_worker_api(
                    worker_snapshot,
                    method="DELETE",
                    path=f"/api/worker/environments/{remote_env_id}",
                )
            except WorkerRequestError as cleanup_error:
                logger.warning(
                    "Failed to cleanup remote worker environment %s after local create failure: %s",
                    remote_env_id,
                    cleanup_error,
                )

        gpu_indices = [int(idx) for idx in (remote_env.get("gpu_indices") or env.selected_gpu_indices or [])]
        custom_ports = _normalize_custom_ports(remote_env.get("custom_ports") or env.custom_ports)
        _validate_custom_ports(custom_ports)

        # Ensure a clean transaction boundary before begin() retries.
        await db.rollback()

        try:
            created_env = None
            for _ in range(MAX_PORT_ALLOCATION_RETRIES):
                try:
                    async with db.begin():
                        surrogate_ssh_port, surrogate_jupyter_port, surrogate_code_port = (
                            await _allocate_remote_surrogate_ports(db)
                        )
                        created_env = Environment(
                            id=remote_env_id,
                            name=env.name,
                            worker_server_id=worker_id,
                            container_user=env.container_user,
                            root_password="__redacted__",
                            root_password_encrypted=encrypted_root_password,
                            dockerfile_content=env.dockerfile_content,
                            enable_jupyter=env.enable_jupyter,
                            enable_code_server=env.enable_code_server,
                            mount_config=[m.model_dump() for m in env.mount_config],
                            gpu_indices=gpu_indices,
                            ssh_port=surrogate_ssh_port,
                            jupyter_port=surrogate_jupyter_port,
                            code_port=surrogate_code_port,
                            status=str(remote_env.get("status") or "building"),
                        )
                        db.add(created_env)
                        db.add(Setting(key=f"custom_ports:{created_env.id}", value=json.dumps(custom_ports)))
                        await db.flush()
                    break
                except IntegrityError as error:
                    await db.rollback()
                    created_env = None
                    if _is_name_unique_violation(error):
                        raise HTTPException(
                            status_code=409,
                            detail={"code": "duplicate_environment_name", "message": "Environment name already exists"},
                        ) from error
                    if _is_port_unique_violation(error):
                        continue
                    raise

            if created_env is None:
                raise HTTPException(
                    status_code=503,
                    detail={
                        "code": "port_allocation_failed",
                        "message": "Failed to allocate unique ports after several retries. Please try again.",
                    },
                )

            env_dict = {**created_env.__dict__, "custom_ports": custom_ports}
            env_dict.pop("_sa_instance_state", None)
            return env_dict
        except Exception:
            await _cleanup_remote_environment()
            raise

    # GPU allocation logic
    gpu_indices: list[int] = []
    requested_indices = [int(idx) for idx in (env.selected_gpu_indices or [])]
    if requested_indices:
        if len(set(requested_indices)) != len(requested_indices):
            raise HTTPException(
                status_code=400,
                detail={"code": "invalid_gpu_selection", "message": "Duplicate GPU indices are not allowed"},
            )

        total_gpus = _detect_total_gpus()
        invalid = sorted(idx for idx in requested_indices if idx < 0 or idx >= total_gpus)
        if invalid:
            raise HTTPException(
                status_code=400,
                detail={"code": "invalid_gpu_selection", "message": f"Invalid GPU indices: {invalid}"},
            )

        gpu_indices = sorted(requested_indices)
    elif env.gpu_count > 0:
        total_gpus = _detect_total_gpus()
        used_indices = await _collect_used_gpu_indices(db, env.worker_server_id)
        available_indices = [i for i in range(total_gpus) if i not in used_indices]
        if len(available_indices) < env.gpu_count:
            raise HTTPException(
                status_code=400,
                detail={
                    "code": "gpu_capacity_insufficient",
                    "message": f"Not enough GPUs available. Requested: {env.gpu_count}, Available: {len(available_indices)}",
                },
            )
        gpu_indices = available_indices[: env.gpu_count]

    custom_ports = _normalize_custom_ports(env.custom_ports)
    _validate_custom_ports(custom_ports)
    blocked_host_ports = await _collect_blocked_host_ports(db)
    for mapping in custom_ports:
        if mapping["host_port"] in blocked_host_ports:
            raise HTTPException(
                status_code=409,
                detail={
                    "code": "custom_host_port_conflict",
                    "message": f"Custom host port {mapping['host_port']} is already in use. Please regenerate ports.",
                },
            )

    await db.rollback()

    new_env = None
    jupyter_token = secrets.token_urlsafe(32)
    for _ in range(MAX_PORT_ALLOCATION_RETRIES):
        try:
            async with db.begin():
                # Serialize GPU allocation + environment insert.
                await db.execute(
                    text("SELECT pg_advisory_xact_lock(:lock_key)"),
                    {"lock_key": GPU_ALLOCATION_LOCK_KEY},
                )

                if requested_indices:
                    latest_used_indices = await _collect_used_gpu_indices(db, env.worker_server_id)
                    conflicted = sorted(idx for idx in requested_indices if idx in latest_used_indices)
                    if conflicted:
                        raise HTTPException(
                            status_code=409,
                            detail={
                                "code": "gpu_already_allocated",
                                "message": f"Requested GPUs are already in use: {conflicted}",
                            },
                        )
                    gpu_indices = sorted(requested_indices)
                elif env.gpu_count > 0:
                    total_gpus = _detect_total_gpus()
                    latest_used_indices = await _collect_used_gpu_indices(db, env.worker_server_id)
                    available_indices = [i for i in range(total_gpus) if i not in latest_used_indices]
                    if len(available_indices) < env.gpu_count:
                        raise HTTPException(
                            status_code=409,
                            detail={
                                "code": "gpu_already_allocated",
                                "message": "Not enough GPUs available at creation time",
                            },
                        )
                    gpu_indices = available_indices[: env.gpu_count]

                try:
                    ssh_port, jupyter_port, code_port = await _allocate_ports(db)
                except HTTPException as port_error:
                    if port_error.status_code == 503:
                        raise HTTPException(
                            status_code=503,
                            detail={
                                "code": "port_allocation_failed",
                                "message": "Failed to allocate unique ports. Please try again.",
                            },
                        ) from port_error
                    raise

                candidate_env = Environment(
                    name=env.name,
                    container_user=env.container_user,
                    root_password="__redacted__",
                    root_password_encrypted=encrypted_root_password,
                    dockerfile_content=env.dockerfile_content,
                    enable_jupyter=env.enable_jupyter,
                    enable_code_server=env.enable_code_server,
                    mount_config=[m.dict() for m in env.mount_config],
                    gpu_indices=gpu_indices,
                    ssh_port=ssh_port,
                    jupyter_port=jupyter_port,
                    code_port=code_port,
                    status="creating",
                )
                db.add(candidate_env)
                await db.flush()

                db.add(Setting(key=f"jupyter_token:{candidate_env.id}", value=jupyter_token))
                db.add(Setting(key=f"custom_ports:{candidate_env.id}", value=json.dumps(custom_ports)))
                new_env = candidate_env

            break
        except IntegrityError as error:
            await db.rollback()
            if _is_name_unique_violation(error):
                raise HTTPException(
                    status_code=409,
                    detail={"code": "duplicate_environment_name", "message": "Environment name already exists"},
                ) from error

    if new_env is None:
        raise HTTPException(
            status_code=503,
            detail={
                "code": "port_allocation_failed",
                "message": "Failed to allocate unique ports after several retries. Please try again.",
            },
        )

    try:
        create_environment_task.delay(str(new_env.id))
    except Exception as enqueue_error:
        compensation_done = False
        try:
            async with db.begin():
                rollback_env = await db.execute(select(Environment).where(Environment.id == new_env.id))
                env_to_remove = rollback_env.scalars().first()
                if env_to_remove:
                    token_result = await db.execute(select(Setting).where(Setting.key == f"jupyter_token:{new_env.id}"))
                    token_setting = token_result.scalars().first()
                    if token_setting:
                        await db.delete(token_setting)

                    custom_ports_result = await db.execute(
                        select(Setting).where(Setting.key == f"custom_ports:{new_env.id}")
                    )
                    custom_ports_setting = custom_ports_result.scalars().first()
                    if custom_ports_setting:
                        await db.delete(custom_ports_setting)

                    await db.delete(env_to_remove)
                compensation_done = True
        except Exception:
            await db.rollback()

        if not compensation_done:
            try:
                async with db.begin():
                    failed_env_result = await db.execute(select(Environment).where(Environment.id == new_env.id))
                    failed_env = failed_env_result.scalars().first()
                    if failed_env:
                        failed_env.status = "error"
            except Exception:
                await db.rollback()

        raise HTTPException(
            status_code=503,
            detail={
                "code": "task_enqueue_failed",
                "message": "Failed to enqueue provisioning task. Please try again.",
            },
        ) from enqueue_error

    async with db.begin():
        refreshed = await db.execute(select(Environment).where(Environment.id == new_env.id))
        created_env = refreshed.scalars().first()
        if created_env:
            created_env.status = "building"
            new_env = created_env

    env_dict = {**new_env.__dict__, "custom_ports": custom_ports}
    env_dict.pop("_sa_instance_state", None)
    return env_dict


def _resolve_environment_status(
    current_status: str,
    container_status: str,
    state_status: str,
    exit_code,
    oom_killed: bool,
    error_msg: str,
) -> str:
    if container_status == "running":
        if current_status == "starting":
            return "running"
        if current_status == "stopping":
            return "stopping"
        if current_status != "running":
            return "running"
        return current_status

    if current_status == "stopping":
        if state_status in ["created", "restarting", "starting"]:
            return "stopping"
        return "stopped"

    if current_status == "starting":
        if state_status in ["created", "restarting", "starting"] and exit_code is None:
            return "starting"
        if exit_code is None:
            return "stopped"
        if exit_code in [0, 143]:
            return "stopped"
        if exit_code == 137:
            if oom_killed or str(error_msg).strip():
                return "error"
            return "stopped"
        return "error"

    if exit_code is None:
        return "stopped"
    if exit_code in [0, 143]:
        return "stopped"
    if exit_code == 137:
        if oom_killed or str(error_msg).strip():
            return "error"
        return "stopped"
    return "error"


@router.get("/", response_model=List[EnvironmentResponse])
async def read_environments(skip: int = 0, limit: int = 100, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).offset(skip).limit(limit))
    envs = result.scalars().all()
    custom_ports_map = await _get_custom_ports_map(db)
    worker_ids = {getattr(env, "worker_server_id", None) for env in envs if getattr(env, "worker_server_id", None)}
    worker_map: dict[UUID, WorkerServer] = {}
    if worker_ids:
        workers_result = await db.execute(select(WorkerServer).where(WorkerServer.id.in_(worker_ids)))
        workers = workers_result.scalars().all()
        worker_map = {worker.id: worker for worker in workers}

    worker_health_cache: dict[UUID, bool] = {}
    worker_health_message_cache: dict[UUID, str] = {}

    client = None
    docker_available = True
    try:
        client = docker.from_env()
    except docker.errors.DockerException as error:
        docker_available = False
        logger.warning("Docker daemon unavailable while reading environments: %s", error)
    except Exception as error:
        docker_available = False
        logger.warning("Unexpected Docker client initialization failure: %s", error)

    env_responses = []

    for env in envs:
        container_name = f"lyra-{env.name}-{env.id}"
        container_id: str | None = None
        response_status = env.status

        env_worker_server_id = getattr(env, "worker_server_id", None)
        if env_worker_server_id:
            worker = worker_map.get(env_worker_server_id)
            if not worker:
                env_dict = {
                    **env.__dict__,
                    "status": "unknown",
                    "worker_server_name": None,
                    "worker_server_base_url": None,
                    "worker_error_code": "worker_not_found",
                    "worker_error_message": "Worker server not found",
                    "container_id": None,
                    "custom_ports": custom_ports_map.get(str(env.id), []),
                }
                env_dict.pop("_sa_instance_state", None)
                env_responses.append(env_dict)
                continue

            healthy = worker_health_cache.get(worker.id)
            if healthy is None:
                health = await refresh_worker_health(db, worker, persist=False)
                healthy = health.status == WORKER_HEALTH_HEALTHY
                worker_health_cache[worker.id] = healthy
                worker_health_message_cache[worker.id] = health.message

            if not healthy:
                env_dict = {
                    **env.__dict__,
                    "status": "unknown",
                    "worker_server_name": worker.name,
                    "worker_server_base_url": worker.base_url,
                    "worker_error_code": f"worker_health_{worker.last_health_status}",
                    "worker_error_message": worker_health_message_cache.get(worker.id)
                    or "Worker server is unreachable",
                    "container_id": None,
                    "custom_ports": custom_ports_map.get(str(env.id), []),
                }
                env_dict.pop("_sa_instance_state", None)
                env_responses.append(env_dict)
                continue

            try:
                remote_env = await call_worker_api(
                    worker,
                    method="GET",
                    path=f"/api/worker/environments/{env.id}",
                )
                remote_status = str(remote_env.get("status") or env.status)
                response_status = remote_status
                container_id = remote_env.get("container_id")
                if isinstance(container_id, str) and len(container_id) > 12:
                    container_id = container_id[:12]
            except WorkerRequestError as error:
                response_status = "unknown"
                worker_error_code = error.code
                worker_error_message = error.message
                container_id = None
            else:
                worker_error_code = None
                worker_error_message = None

            env_dict = {
                **env.__dict__,
                "status": response_status,
                "worker_server_name": worker.name,
                "worker_server_base_url": worker.base_url,
                "worker_error_code": worker_error_code,
                "worker_error_message": worker_error_message,
                "container_id": container_id,
                "custom_ports": custom_ports_map.get(str(env.id), []),
            }
            env_dict.pop("_sa_instance_state", None)
            env_responses.append(env_dict)
            continue

        if docker_available and client is not None:
            try:
                container = client.containers.get(container_name)
                container_id = container.short_id or (container.id[:12] if container.id else None)
                state_info = container.attrs.get("State", {})
                new_status = _resolve_environment_status(
                    current_status=env.status,
                    container_status=container.status,
                    state_status=state_info.get("Status", ""),
                    exit_code=state_info.get("ExitCode"),
                    oom_killed=state_info.get("OOMKilled", False),
                    error_msg=state_info.get("Error", ""),
                )
                response_status = new_status
            except docker.errors.NotFound:
                if env.status in ["running", "stopping", "starting"]:
                    response_status = "stopped"
            except docker.errors.DockerException as error:
                logger.warning(
                    "Docker status lookup failed for env %s. Falling back to DB status: %s",
                    env.id,
                    error,
                )
            except Exception as error:
                logger.warning(
                    "Unexpected status resolution failure for env %s. Falling back to DB status: %s",
                    env.id,
                    error,
                )

        env_dict = {
            **env.__dict__,
            "status": response_status,
            "worker_server_name": None,
            "worker_server_base_url": None,
            "worker_error_code": None,
            "worker_error_message": None,
            "container_id": container_id,
            "custom_ports": custom_ports_map.get(str(env.id), []),
        }
        env_dict.pop("_sa_instance_state", None)
        env_responses.append(env_dict)

    return env_responses


@router.get("/{environment_id}", response_model=EnvironmentResponse)
async def read_environment(environment_id: str, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")

    worker_server_name = None
    worker_server_base_url = None
    worker_error_code = None
    worker_error_message = None
    response_status = env.status
    container_id: str | None = None

    if env.worker_server_id:
        worker = await _get_worker_server_by_id(db, env.worker_server_id)
        if worker:
            worker_server_name = worker.name
            worker_server_base_url = worker.base_url
            try:
                remote_env = await call_worker_api(
                    worker,
                    method="GET",
                    path=f"/api/worker/environments/{env.id}",
                )
                remote_status = str(remote_env.get("status") or env.status)
                response_status = remote_status
                remote_container_id = remote_env.get("container_id")
                if isinstance(remote_container_id, str) and remote_container_id.strip():
                    container_id = remote_container_id[:12]
            except WorkerRequestError as error:
                response_status = "unknown"
                worker_error_code = error.code
                worker_error_message = error.message
        else:
            response_status = "unknown"
            worker_error_code = "worker_not_found"
            worker_error_message = "Worker server not found"
    else:
        container_name = f"lyra-{env.name}-{env.id}"
        try:
            client = docker.from_env()
            container = client.containers.get(container_name)
            container_id = container.short_id or (container.id[:12] if container.id else None)
            state_info = container.attrs.get("State", {})
            response_status = _resolve_environment_status(
                current_status=env.status,
                container_status=container.status,
                state_status=state_info.get("Status", ""),
                exit_code=state_info.get("ExitCode"),
                oom_killed=state_info.get("OOMKilled", False),
                error_msg=state_info.get("Error", ""),
            )
            if isinstance(container_id, str) and len(container_id) > 12:
                container_id = container_id[:12]
        except docker.errors.NotFound:
            container_id = None
            if env.status in ["running", "stopping", "starting"]:
                response_status = "stopped"
        except Exception as error:
            logger.warning("Failed to resolve container id for environment %s: %s", env.id, error)
            container_id = None

    custom_ports = await _get_custom_ports_for_environment(db, str(env.id))
    env_dict = {
        **env.__dict__,
        "status": response_status,
        "custom_ports": custom_ports,
        "worker_server_name": worker_server_name,
        "worker_server_base_url": worker_server_base_url,
        "worker_error_code": worker_error_code,
        "worker_error_message": worker_error_message,
        "container_id": container_id,
    }
    env_dict.pop("_sa_instance_state", None)
    return env_dict


@router.get("/{environment_id}/logs")
async def get_environment_logs(environment_id: str, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")

    if env.worker_server_id:
        worker = await _assert_worker_is_ready(db, env.worker_server_id)
        try:
            return await call_worker_api(
                worker,
                method="GET",
                path=f"/api/worker/environments/{env.id}/logs",
            )
        except WorkerRequestError as error:
            raise _map_worker_request_error(error) from error

    client = docker.from_env()
    container_name = f"lyra-{env.name}-{env.id}"

    try:
        # Check if container exists (even if stopped/exited)
        container = client.containers.get(container_name)
        logs = container.logs(tail=50)
        logs_text = logs.decode('utf-8').strip() if logs else ""
        state_summary = _format_container_state_summary(container)

        if not logs_text:
            return {"logs": f"{state_summary}\n\nNo logs produced by this container."}

        # For failed/stopped containers, include state diagnostics above recent logs.
        if env.status == "error" or container.status in {"exited", "dead"}:
            return {"logs": f"{state_summary}\n\n[Recent Logs]\n{logs_text}"}

        return {"logs": logs_text}
    except docker.errors.NotFound:
        if env.status == "error":
            build_error_message = await _get_build_error_message(db, str(env.id))
            if build_error_message:
                return {
                    "logs": (
                        "No container was created for this environment. Build may have failed before container start.\n\n"
                        "[Build Failure Details]\n"
                        f"{build_error_message}"
                    )
                }
            return {
                "logs": "No container was created for this environment. Build may have failed before container start. Check backend worker logs for the full build error."  # noqa: E501
            }
        return {"logs": "Container not found. It may have been removed or not started yet."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/{environment_id}/jupyter/launch")
async def create_jupyter_launch_url(environment_id: str, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")
    if not env.enable_jupyter:
        raise HTTPException(status_code=409, detail="Jupyter is disabled for this environment")

    if env.worker_server_id:
        worker = await _assert_worker_is_ready(db, env.worker_server_id)
        try:
            worker_launch = await call_worker_api(
                worker,
                method="POST",
                path=f"/api/worker/environments/{env.id}/jupyter/launch",
            )
        except WorkerRequestError as error:
            raise _map_worker_request_error(error) from error

        remote_launch_path = str(worker_launch.get("launch_url") or "").strip()
        worker_service_port = _parse_worker_service_port(worker_launch.get("port"))
        if not remote_launch_path:
            raise HTTPException(
                status_code=502,
                detail={"code": "worker_api_mismatch", "message": "Worker launch URL response is invalid"},
            )
        if worker_service_port is not None:
            remote_launch_url = _build_worker_service_url(worker.base_url, worker_service_port, remote_launch_path)
        elif remote_launch_path.startswith("http://") or remote_launch_path.startswith("https://"):
            remote_launch_url = remote_launch_path
        else:
            base_url = str(worker.base_url or "").rstrip("/")
            if not remote_launch_path.startswith("/"):
                remote_launch_path = f"/{remote_launch_path}"
            remote_launch_url = f"{base_url}{remote_launch_path}"

        _cleanup_expired_jupyter_tickets()
        launch_ticket = secrets.token_urlsafe(24)
        jupyter_launch_tickets[launch_ticket] = {
            "environment_id": str(env.id),
            "expires_at": time.time() + JUPYTER_LAUNCH_TTL_SECONDS,
            "used": False,
            "remote_launch_url": remote_launch_url,
        }
        return {"launch_url": f"/api/environments/{environment_id}/jupyter/launch/{launch_ticket}"}

    if env.status != "running" and not _is_host_environment_running_now(env):
        raise HTTPException(status_code=409, detail="Environment must be running")
    if env.status != "running":
        env.status = "running"
        await db.commit()

    token = await _get_jupyter_token(db, str(env.id))
    if not token:
        raise HTTPException(status_code=409, detail="Jupyter token is not configured. Recreate the environment.")

    _cleanup_expired_jupyter_tickets()
    launch_ticket = secrets.token_urlsafe(24)
    jupyter_launch_tickets[launch_ticket] = {
        "environment_id": str(env.id),
        "expires_at": time.time() + JUPYTER_LAUNCH_TTL_SECONDS,
        "used": False,
    }

    return {"launch_url": f"/api/environments/{environment_id}/jupyter/launch/{launch_ticket}"}


@router.get("/{environment_id}/jupyter/launch/{launch_ticket}")
async def launch_jupyter_with_ticket(
    environment_id: str, launch_ticket: str, request: Request, db: AsyncSession = Depends(get_db)
):
    _cleanup_expired_jupyter_tickets()
    ticket_meta = jupyter_launch_tickets.get(launch_ticket)
    if not ticket_meta:
        raise HTTPException(status_code=404, detail="Launch ticket not found or expired")
    if ticket_meta.get("used"):
        raise HTTPException(status_code=410, detail="Launch ticket already used")
    if ticket_meta.get("environment_id") != environment_id:
        raise HTTPException(status_code=400, detail="Launch ticket does not match environment")
    if float(ticket_meta.get("expires_at", 0)) < time.time():
        jupyter_launch_tickets.pop(launch_ticket, None)
        raise HTTPException(status_code=410, detail="Launch ticket expired")

    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")
    if not env.enable_jupyter:
        raise HTTPException(status_code=409, detail="Jupyter is disabled for this environment")

    remote_launch_url = str(ticket_meta.get("remote_launch_url") or "").strip()
    if env.worker_server_id and remote_launch_url:
        ticket_meta["used"] = True
        return RedirectResponse(url=remote_launch_url, status_code=307)

    if env.status != "running" and not _is_host_environment_running_now(env):
        raise HTTPException(status_code=409, detail="Environment must be running")
    if env.status != "running":
        env.status = "running"
        await db.commit()

    token = await _get_jupyter_token(db, str(env.id))
    if not token:
        raise HTTPException(status_code=409, detail="Jupyter token is not configured. Recreate the environment.")

    ticket_meta["used"] = True

    scheme = request.headers.get("x-forwarded-proto", request.url.scheme)
    host = request.url.hostname or "localhost"
    redirect_url = f"{scheme}://{host}:{env.jupyter_port}/?token={token}"
    return RedirectResponse(url=redirect_url, status_code=307)


@router.post("/{environment_id}/code/launch")
async def create_code_launch_url(environment_id: str, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")
    if not env.enable_code_server:
        raise HTTPException(status_code=409, detail="code-server is disabled for this environment")

    if env.worker_server_id:
        worker = await _assert_worker_is_ready(db, env.worker_server_id)
        try:
            worker_launch = await call_worker_api(
                worker,
                method="POST",
                path=f"/api/worker/environments/{env.id}/code/launch",
            )
        except WorkerRequestError as error:
            raise _map_worker_request_error(error) from error

        remote_launch_path = str(worker_launch.get("launch_url") or "").strip()
        worker_service_port = _parse_worker_service_port(worker_launch.get("port"))
        if not remote_launch_path:
            raise HTTPException(
                status_code=502,
                detail={"code": "worker_api_mismatch", "message": "Worker launch URL response is invalid"},
            )
        if worker_service_port is not None:
            remote_launch_url = _build_worker_service_url(worker.base_url, worker_service_port, remote_launch_path)
        elif remote_launch_path.startswith("http://") or remote_launch_path.startswith("https://"):
            remote_launch_url = remote_launch_path
        else:
            base_url = str(worker.base_url or "").rstrip("/")
            if not remote_launch_path.startswith("/"):
                remote_launch_path = f"/{remote_launch_path}"
            remote_launch_url = f"{base_url}{remote_launch_path}"

        _cleanup_expired_code_tickets()
        launch_ticket = secrets.token_urlsafe(24)
        code_launch_tickets[launch_ticket] = {
            "environment_id": str(env.id),
            "expires_at": time.time() + CODE_LAUNCH_TTL_SECONDS,
            "used": False,
            "remote_launch_url": remote_launch_url,
        }
        return {"launch_url": f"/api/environments/{environment_id}/code/launch/{launch_ticket}"}

    if env.status != "running" and not _is_host_environment_running_now(env):
        raise HTTPException(status_code=409, detail="Environment must be running")
    if env.status != "running":
        env.status = "running"
        await db.commit()

    _cleanup_expired_code_tickets()
    launch_ticket = secrets.token_urlsafe(24)
    code_launch_tickets[launch_ticket] = {
        "environment_id": str(env.id),
        "expires_at": time.time() + CODE_LAUNCH_TTL_SECONDS,
        "used": False,
    }
    return {"launch_url": f"/api/environments/{environment_id}/code/launch/{launch_ticket}"}


@router.get("/{environment_id}/code/launch/{launch_ticket}")
async def launch_code_with_ticket(
    environment_id: str, launch_ticket: str, request: Request, db: AsyncSession = Depends(get_db)
):
    _cleanup_expired_code_tickets()
    ticket_meta = code_launch_tickets.get(launch_ticket)
    if not ticket_meta:
        raise HTTPException(status_code=404, detail="Launch ticket not found or expired")
    if ticket_meta.get("used"):
        raise HTTPException(status_code=410, detail="Launch ticket already used")
    if ticket_meta.get("environment_id") != environment_id:
        raise HTTPException(status_code=400, detail="Launch ticket does not match environment")
    if float(ticket_meta.get("expires_at", 0)) < time.time():
        code_launch_tickets.pop(launch_ticket, None)
        raise HTTPException(status_code=410, detail="Launch ticket expired")

    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")
    if not env.enable_code_server:
        raise HTTPException(status_code=409, detail="code-server is disabled for this environment")

    remote_launch_url = str(ticket_meta.get("remote_launch_url") or "").strip()
    if env.worker_server_id and remote_launch_url:
        ticket_meta["used"] = True
        return RedirectResponse(url=remote_launch_url, status_code=307)

    if env.status != "running" and not _is_host_environment_running_now(env):
        raise HTTPException(status_code=409, detail="Environment must be running")
    if env.status != "running":
        env.status = "running"
        await db.commit()

    ticket_meta["used"] = True
    scheme = request.headers.get("x-forwarded-proto", request.url.scheme)
    host = request.url.hostname or "localhost"
    redirect_url = f"{scheme}://{host}:{env.code_port}"
    return RedirectResponse(url=redirect_url, status_code=307)


@router.delete("/{environment_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_environment(
    environment_id: str,
    force: bool = Query(default=False),
    db: AsyncSession = Depends(get_db),
):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")

    remote_delete_completed = False
    if env.worker_server_id:
        logger.info("Delete stage(remote) started for environment %s on worker %s", env.id, env.worker_server_id)
        if force:
            worker = await _get_worker_server_by_id(db, env.worker_server_id)
            if worker:
                try:
                    await call_worker_api(
                        worker,
                        method="DELETE",
                        path=f"/api/worker/environments/{env.id}",
                    )
                    remote_delete_completed = True
                except WorkerRequestError:
                    # Force delete should still allow local cleanup for orphaned environments.
                    pass
        else:
            worker = await _assert_worker_is_ready(db, env.worker_server_id)
            try:
                await call_worker_api(
                    worker,
                    method="DELETE",
                    path=f"/api/worker/environments/{env.id}",
                )
                remote_delete_completed = True
            except WorkerRequestError as error:
                # Treat "already missing on worker" as idempotent success and continue local cleanup.
                if _is_worker_environment_not_found(error):
                    logger.info(
                        "Worker environment %s was already missing during delete; continuing local cleanup.",
                        env.id,
                    )
                    remote_delete_completed = True
                else:
                    # In some race cases, delete can fail but the environment is already gone on worker.
                    if await _is_worker_environment_absent(worker, str(env.id)):
                        logger.info(
                            "Worker environment %s not found after delete error; continuing local cleanup.",
                            env.id,
                        )
                        remote_delete_completed = True
                    else:
                        raise _map_worker_request_error(error) from error

    # Try to remove container.
    # For host environments, container removal failures should fail deletion.
    # For worker-bound environments on main, ignore local daemon issues.
    try:
        client = docker.from_env()
        container_name = f"lyra-{env.name}-{env.id}"
        try:
            container = client.containers.get(container_name)
            container.remove(force=True)
        except docker.errors.NotFound:
            pass  # Container already gone
    except Exception as e:
        if env.worker_server_id is None:
            raise HTTPException(
                status_code=500,
                detail={"code": "container_delete_failed", "message": f"Failed to remove container: {e}"},
            ) from e
        logger.warning("Error removing local container while deleting worker-bound environment: %s", e)

    logger.info("Delete stage(local-db) started for environment %s", env.id)
    try:
        token_key = f"jupyter_token:{env.id}"
        token_result = await db.execute(select(Setting).where(Setting.key == token_key))
        token_setting = token_result.scalars().first()
        if token_setting:
            await db.delete(token_setting)

        custom_ports_key = f"custom_ports:{env.id}"
        custom_ports_result = await db.execute(select(Setting).where(Setting.key == custom_ports_key))
        custom_ports_setting = custom_ports_result.scalars().first()
        if custom_ports_setting:
            await db.delete(custom_ports_setting)

        build_error_key = f"{BUILD_ERROR_SETTING_PREFIX}{env.id}"
        build_error_result = await db.execute(select(Setting).where(Setting.key == build_error_key))
        build_error_setting = build_error_result.scalars().first()
        if build_error_setting:
            await db.delete(build_error_setting)

        await db.delete(env)
        await db.commit()
    except Exception as error:
        await db.rollback()
        logger.exception("Delete stage(local-db) failed for environment %s: %s", env.id, error)
        if env.worker_server_id and remote_delete_completed:
            raise HTTPException(
                status_code=500,
                detail={
                    "code": "local_cleanup_failed",
                    "message": "Worker environment was deleted, but local cleanup failed. Retry delete with force.",
                },
            ) from error
        raise

    logger.info("Delete completed for environment %s", env.id)
    _cleanup_expired_jupyter_tickets()
    _cleanup_expired_code_tickets()

    return None


@router.post("/{environment_id}/start", status_code=status.HTTP_200_OK)
async def start_environment(environment_id: str, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")

    if env.worker_server_id:
        worker = await _assert_worker_is_ready(db, env.worker_server_id)
        try:
            response = await call_worker_api(
                worker,
                method="POST",
                path=f"/api/worker/environments/{env.id}/start",
            )
            env.status = "running"
            await db.commit()
            return response
        except WorkerRequestError as error:
            env.status = "error"
            await db.commit()
            raise _map_worker_request_error(error) from error

    container_name = f"lyra-{env.name}-{env.id}"
    client = docker.from_env()

    try:
        container = client.containers.get(container_name)
        if container.status == "running":
            env.status = "running"
            await db.commit()
            return {"message": "Environment is already running"}

        env.status = "starting"
        await db.commit()
        container.start()
        env.status = "running"
        await db.commit()
        return {"message": f"Environment {env.name} started"}
    except docker.errors.NotFound:
        env.status = "error"
        await db.commit()
        raise HTTPException(status_code=409, detail="Container not found. Please recreate the environment.")
    except Exception as e:
        env.status = "error"
        await db.commit()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/{environment_id}/stop", status_code=status.HTTP_200_OK)
async def stop_environment(environment_id: str, db: AsyncSession = Depends(get_db)):
    result = await db.execute(select(Environment).where(Environment.id == environment_id))
    env = result.scalars().first()
    if env is None:
        raise HTTPException(status_code=404, detail="Environment not found")

    if env.worker_server_id:
        worker = await _assert_worker_is_ready(db, env.worker_server_id)
        try:
            response = await call_worker_api(
                worker,
                method="POST",
                path=f"/api/worker/environments/{env.id}/stop",
            )
            env.status = "stopping"
            await db.commit()
            return response
        except WorkerRequestError as error:
            env.status = "error"
            await db.commit()
            raise _map_worker_request_error(error) from error

    container_name = f"lyra-{env.name}-{env.id}"
    client = docker.from_env()

    try:
        container = client.containers.get(container_name)
        if container.status != "running":
            env.status = "stopped"
            await db.commit()
            return {"message": "Environment is already stopped"}

        env.status = "stopping"
        await db.commit()
        container.stop(timeout=0)
        return {"message": f"Environment {env.name} is stopping"}
    except docker.errors.NotFound:
        env.status = "stopped"
        await db.commit()
        return {"message": "Container not found. Environment marked as stopped."}
    except Exception as e:
        env.status = "error"
        await db.commit()
        raise HTTPException(status_code=500, detail=str(e))
